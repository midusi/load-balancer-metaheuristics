import time
from copy import deepcopy
from typing import Literal
import binpacking
import numpy as np
from metaheuristics import get_random_subset_of_features
import matplotlib.pyplot as plt

# Dict with the partition identifier as key and the worker identifier on which the partition will be computed as value
PartitionToWorker = dict[int, str]

# Dict with the worker identifier as key and the list of execution times as value
WorkerTimes = dict[str, list[float]]

# Types of strategy to define partitions
PartitionStrategy = Literal['n_stars', 'binpacking', 'smart']

# Strategy to define partitions
STRATEGY: PartitionStrategy = 'n_stars'

# TODO: implement random_seed to replicate experiments

# Some constants
N_WORKERS = 3
N_STARS = 6
N_FEATURES = 5

# To print useful information
DEBUG = True


class Rdd:
    """
    Simulates the RDD class of Spark. An RDD has a subset of features and a partition to define in which worker
    it will be computed.
    """
    partition: int
    subset: np.ndarray

    def __init__(self, partition: int, subset: np.ndarray):
        self.partition = partition
        self.subset = subset

    def __repr__(self):
        return f"Rdd(partition={self.partition}, n_features={np.count_nonzero(self.subset)})"


def collect(rdds: list[Rdd], partition_to_worker: PartitionToWorker):
    """
    Simulates the collect() method of Spark.
    :param rdds: List of RDD to evaluate.
    :param partition_to_worker: Dict with the partition identifier as key and the worker identifier on which the
    partition will be computed as value.
    """
    worker_execution_times: WorkerTimes = {}

    for rdd in rdds:
        worker_id = partition_to_worker[rdd.partition]

        worker_start = time.time()
        _current_fitness = __fitness_function(rdd.subset)
        worker_end = time.time()

        worker_execution_time = worker_end - worker_start

        # Stores the execution time of the worker
        if worker_id in worker_execution_times:
            worker_execution_times[worker_id].append(worker_execution_time)
        else:
            worker_execution_times[worker_id] = [worker_execution_time]

    # Gets max sum of execution times
    max_worker_time = -1
    for worker_id in worker_execution_times:
        sum_worker_time = np.sum(worker_execution_times[worker_id])

        if DEBUG:
            print(f'Worker {worker_id} has executed {len(worker_execution_times[worker_id])} RDDs in '
                  f'{sum_worker_time} seconds')

        if sum_worker_time > max_worker_time:
            max_worker_time = sum_worker_time

    # Stores the idle time for every worker. This is the difference between the max worker time and the sum
    # of the execution times of the worker
    idle_times: WorkerTimes = {
        worker_id: max_worker_time - np.sum(worker_execution_times[worker_id])
        for worker_id in worker_execution_times
    }

    return worker_execution_times, idle_times


def __fitness_function(subset: np.ndarray) -> int:
    """Simulates a fitness function. It will return the number of features in the subset, and it'll last the same amount
    of time as the number of features in the subset."""
    time.sleep(np.count_nonzero(subset) / 9)
    return len(subset)


def __predict(subset: np.ndarray) -> np.ndarray:
    """Simulates the predict function. It will return the number of features +- some random epsilon."""
    return np.count_nonzero(subset) + np.random.uniform(-1, 1)


def __generate_stars_and_partitions_bins(bins: list) -> dict[int, int]:
    """
    Generates a dict with the idx of the star and the assigned partition
    :param bins: Bins generated by binpacking
    :return: Dict where keys are star index, values are the Spark partition
    """
    stars_and_partitions: dict[int, int] = {}
    for partition_id, aux_bin in enumerate(bins):
        for star_idx in aux_bin.keys():
            stars_and_partitions[star_idx] = partition_id
    return stars_and_partitions


def __binpacking_strategy(rdds: list[Rdd]) -> list[Rdd]:
    res_rdd = deepcopy(rdds)
    stars_and_times: dict[str, float] = {idx: __predict(rdd.subset) for (idx, rdd) in enumerate(res_rdd)}
    bins = binpacking.to_constant_bin_number(stars_and_times, N_WORKERS)  # n_workers is the number of bins

    if DEBUG:
        print("Stars (keys) and their predicted execution times (values):")
        print(f"\nRepartition among {N_WORKERS} bins:")
        print(bins)

    # Generates a dict with the idx of the star and the assigned partition
    stars_and_partitions = __generate_stars_and_partitions_bins(bins)

    if DEBUG:
        print(stars_and_partitions)

    # Assigns the partition to each RDD
    for idx, rdd in enumerate(res_rdd):
        rdd.partition = stars_and_partitions[idx]

    return res_rdd


def __assign_partitions(rdds: list[Rdd], strategy: PartitionStrategy) -> list[Rdd]:
    """Assigns the partition to each RDD depending on the strategy. TODO: implement smart strategy"""
    if strategy == 'binpacking':
        return __binpacking_strategy(rdds)
    if strategy == 'n_stars':
        # Sepa
        len_rdds = len(rdds)
        return [Rdd(i * N_WORKERS // len(rdds), rdds[i].subset) for i in range(len_rdds)]


def main():
    rdds: list[Rdd] = []
    for i in range(N_STARS):
        random_features_to_select = get_random_subset_of_features(N_FEATURES)
        rdd = Rdd(i, random_features_to_select)
        rdds.append(rdd)

    if DEBUG:
        print('rdds before assign partitions')
        print(rdds)

    # Assigns the partition to each RDD
    rdds = __assign_partitions(rdds, STRATEGY)

    if DEBUG:
        print('rdds after assign partitions')
        print(rdds)

    # Generates partition to worker dict
    partition_to_worker: PartitionToWorker = {partition_id: f'worker_{partition_id}' for partition_id in
                                              range(N_WORKERS)}

    # Executes the simulation
    worker_execution_times, worker_idle_times = collect(rdds, partition_to_worker)

    if DEBUG:
        print(f'worker_execution_times: {worker_execution_times}')
        print(f'idle_times: {worker_idle_times}')

    # Plots the execution times in bar chart for each worker
    __plot_bars('Execution', worker_execution_times)
    __plot_bars('Idle', worker_idle_times)
    plt.show()


def __plot_bars(title: Literal['Execution', 'Idle'], times: dict[str, list[float]]):
    """Plots the execution/idle times in bar chart for each worker."""
    values = [np.sum(values) for values in times.values()]
    fig, ax = plt.subplots()
    plt.bar(times.keys(), values)
    ax.set_title(f'{title} times for each worker')
    ax.set_xlabel('Worker')
    ax.set_ylabel('Execution time (s)')

    # Shows value on top of each bar
    for index, value in enumerate(values):
        plt.text(index, value, str(round(value, 2)), ha='center', va='bottom')

    plt.legend()


if __name__ == '__main__':
    main()
