import time
from copy import deepcopy
from typing import Literal, Final, Optional
import binpacking
import numpy as np
from metaheuristics import get_random_subset_of_features
import matplotlib.pyplot as plt

# Dict with the partition identifier as key and the worker identifier on which the partition will be computed as value
PartitionToWorker = dict[int, str]

# Dict with the worker identifier as key and some delay to apply in every execution. For example, 'worker_0': 1.75
# will generate a delay of 75% in the worker_0 execution time for every RDD.
DelayForWorker = dict[str, float | None]

# Dict with the worker identifier as key and the list of execution times as value
WorkerTimes = dict[str, list[float]]

# Idle/Execution times structure for bar charts. Keys are workers, values is a List of lists with the iteration number
# and the time.
WorkerBarTimes = dict[str, list[list[float]]]

# Types of strategy to define partitions
PartitionStrategy = Literal['n_stars', 'binpacking', 'smart']

# Strategy to define partitions
STRATEGY: Final[PartitionStrategy] = 'binpacking'

# Seed for reproducibility
RANDOM_SEED: Final[Optional[int]] = 12

# If true, adds a delay to worker number 2
ADD_DELAY_TO_WORKER_2: Final[bool] = False

# Some constants
N_WORKERS: Final = 3
N_STARS: Final = 6
N_FEATURES: Final = 5
ITERATIONS: Final = 3

# To print useful information
DEBUG: Final = True


class Rdd:
    """
    Simulates the RDD class of Spark. An RDD has a subset of features and a partition to define in which worker
    it will be computed.
    """
    partition: int
    subset: np.ndarray

    def __init__(self, partition: int, subset: np.ndarray):
        self.partition = partition
        self.subset = subset

    def __repr__(self):
        return f"Rdd(partition={self.partition}, n_features={np.count_nonzero(self.subset)})"


def collect(rdds: list[Rdd], partition_to_worker: PartitionToWorker,
            delay_for_worker: Optional[DelayForWorker]) -> tuple[WorkerTimes, WorkerTimes]:
    """
    Simulates the collect() method of Spark.
    :param rdds: List of RDD to evaluate.
    :param partition_to_worker: Dict with the partition identifier as key and the worker identifier on which the
    partition will be computed as value.
    :param delay_for_worker: Dict with the worker identifier as key and some delay to apply in every execution.
    """
    worker_execution_times: WorkerTimes = {}

    for rdd in rdds:
        worker_id = partition_to_worker[rdd.partition]

        worker_start = time.time()
        _current_fitness = __fitness_function(rdd.subset)
        worker_end = time.time()

        worker_execution_time = worker_end - worker_start

        if delay_for_worker is not None and worker_id in delay_for_worker:
            if DEBUG:
                print(f'Worker {worker_id} has a delay of {delay_for_worker[worker_id]}')
                print(f'Original execution time: {worker_execution_time} | New execution time: '
                      f'{worker_execution_time * delay_for_worker[worker_id]}')
            worker_execution_time *= delay_for_worker[worker_id]

        # Stores the execution time of the worker
        if worker_id in worker_execution_times:
            worker_execution_times[worker_id].append(worker_execution_time)
        else:
            worker_execution_times[worker_id] = [worker_execution_time]

    # Gets max sum of execution times
    max_worker_time = -1
    for worker_id in worker_execution_times:
        sum_worker_time = np.sum(worker_execution_times[worker_id])

        if DEBUG:
            print(f'Worker {worker_id} has executed {len(worker_execution_times[worker_id])} RDDs in '
                  f'{sum_worker_time} seconds')

        if sum_worker_time > max_worker_time:
            max_worker_time = sum_worker_time

    # Stores the idle time for every worker. This is the difference between the max worker time and the sum
    # of the execution times of the worker
    idle_times: WorkerTimes = {
        worker_id: max_worker_time - np.sum(worker_execution_times[worker_id])
        for worker_id in worker_execution_times
    }

    return worker_execution_times, idle_times


def __fitness_function(subset: np.ndarray) -> int:
    """Simulates a fitness function. It will return the number of features in the subset, and it'll last the same amount
    of time as the number of features in the subset."""
    time.sleep(np.count_nonzero(subset) / 9)
    return len(subset)


def __predict(subset: np.ndarray, random_seed: Optional[int]) -> float:
    """Simulates the predict function. It will return the number of features +- some random epsilon."""
    if random_seed is not None:
        np.random.seed(random_seed)

    return np.count_nonzero(subset) + np.random.uniform(-1, 1)


def __generate_stars_and_partitions_bins(bins: list) -> dict[int, int]:
    """
    Generates a dict with the idx of the star and the assigned partition
    :param bins: Bins generated by binpacking
    :return: Dict where keys are star index, values are the Spark partition
    """
    stars_and_partitions: dict[int, int] = {}
    for partition_id, aux_bin in enumerate(bins):
        for star_idx in aux_bin.keys():
            stars_and_partitions[star_idx] = partition_id
    return stars_and_partitions


def __binpacking_strategy(rdds: list[Rdd]) -> list[Rdd]:
    res_rdd = deepcopy(rdds)

    stars_and_times: dict[str, float] = {}
    for (idx, rdd) in enumerate(res_rdd):
        random_seed = RANDOM_SEED + idx if RANDOM_SEED is not None else None
        stars_and_times[idx] = __predict(rdd.subset, random_seed)
    bins = binpacking.to_constant_bin_number(stars_and_times, N_WORKERS)  # n_workers is the number of bins

    if DEBUG:
        print("Stars (keys) and their predicted execution times (values):")
        print(f"\nRepartition among {N_WORKERS} bins:")
        print(bins)

    # Generates a dict with the idx of the star and the assigned partition
    stars_and_partitions = __generate_stars_and_partitions_bins(bins)

    if DEBUG:
        print(stars_and_partitions)

    # Assigns the partition to each RDD
    for idx, rdd in enumerate(res_rdd):
        rdd.partition = stars_and_partitions[idx]

    return res_rdd


def __assign_partitions(rdds: list[Rdd], strategy: PartitionStrategy) -> list[Rdd]:
    """Assigns the partition to each RDD depending on the strategy. TODO: implement smart strategy"""
    if strategy == 'binpacking':
        return __binpacking_strategy(rdds)
    if strategy == 'n_stars':
        # Separates the RDDs in N_STARS groups
        len_rdds = len(rdds)
        return [Rdd(i * N_WORKERS // len(rdds), rdds[i].subset) for i in range(len_rdds)]


def __add_to_worker_dict(current_execution_times: WorkerTimes, time_worker: WorkerBarTimes, iteration: int):
    """Adds the execution/idle times of the current iteration to the dict with the execution/idle times per worker."""
    for worker_id in current_execution_times:
        if worker_id in time_worker:
            time_worker[worker_id].append([iteration, np.sum(current_execution_times[worker_id])])
        else:
            time_worker[worker_id] = [[iteration, np.sum(current_execution_times[worker_id])]]


def generate_bar_charts(data: WorkerBarTimes, title: str, data_type: Literal['Execution', 'Idle']):
    """
    Plots a bar chart
    :param data: Dictionary with the worker name and the number of id
    :param title: Title to show in the bar chart
    :param data_type: To show 'Idle' or 'Execution' in bar chart title.
    """
    _fig, ax = plt.subplots()

    # Adds some text for labels, title and axes ticks
    ax.set_ylabel(f'{data_type} time (seconds)')
    fig_title = f'{data_type} time per worker. {title}'
    ax.set_title(fig_title)

    width = 0.25
    iterations: np.ndarray = np.array([])  # Just to prevent MyPy warning
    for idx, worker in enumerate(data.keys()):
        np_array = np.array(data[worker])
        iterations = np_array[:, 0] + 1  # +1 to start from 1 instead of 0
        data_times_per_iteration = np_array[:, 1]
        # print(f'Worker {worker} | {data_type} times: {data_times_per_iteration}')
        margin = width * idx
        plt.bar(iterations + margin, data_times_per_iteration, width=width, label=worker)

    # Sets 10 as max value for y axis
    plt.ylim(0, 10)
    plt.xticks(iterations)

    # Gets labels and shows them sorted
    _handles, labels = plt.gca().get_legend_handles_labels()
    plt.legend(sorted(labels))


def main():
    rdds: list[Rdd] = []
    execution_times_worker: WorkerBarTimes = {}
    idle_times_worker: WorkerBarTimes = {}

    for iteration in range(ITERATIONS):
        print(f'Iteration {iteration}')
        print('====================================')
        for i in range(N_STARS):
            random_seed = (RANDOM_SEED + i) * (iteration + 1) if RANDOM_SEED is not None else None
            random_features_to_select = get_random_subset_of_features(N_FEATURES, random_state=random_seed)
            rdd = Rdd(i, random_features_to_select)
            rdds.append(rdd)

        if DEBUG:
            print('rdds before assign partitions')
            print(rdds)

        # Assigns the partition to each RDD
        rdds = __assign_partitions(rdds, STRATEGY)

        if DEBUG:
            print('rdds after assign partitions')
            print(rdds)

        # Generates partition to worker dict
        partition_to_worker: PartitionToWorker = {partition_id: f'worker_{partition_id}' for partition_id in
                                                  range(N_WORKERS)}

        # Generates a dict with the worker identifier as key and the delay to apply in every execution as value
        if ADD_DELAY_TO_WORKER_2:
            delay_for_worker: Optional[DelayForWorker] = {
                'worker_2': 1.75,
            }

        # Executes the simulation
        current_execution_times, current_idle_times = collect(rdds, partition_to_worker, delay_for_worker)

        if DEBUG:
            print(f'worker_execution_times: {current_execution_times}')
            print(f'idle_times: {current_idle_times}')

        # Adds the data to execution and idle times
        __add_to_worker_dict(current_execution_times, execution_times_worker, iteration)
        __add_to_worker_dict(current_idle_times, idle_times_worker, iteration)

    generate_bar_charts(execution_times_worker, f'Strategy = {STRATEGY}', 'Execution')
    generate_bar_charts(idle_times_worker, f'Strategy = {STRATEGY}', 'Idle')
    plt.show()


if __name__ == '__main__':
    main()
